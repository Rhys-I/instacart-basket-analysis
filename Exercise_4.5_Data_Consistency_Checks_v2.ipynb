{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6464fa81",
   "metadata": {},
   "source": [
    "# Exercise 4.5: Data Consistency Checks\n",
    "\n",
    "## ðŸ“‹ Table of Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Import Libraries & Data](#Import)\n",
    "3. [Check for Mixed Data Types](#MixedTypes)\n",
    "4. [Check for Missing Values](#MissingValues)\n",
    "5. [Handle Missing Values](#HandleMissing)\n",
    "6. [Check for Duplicates](#Duplicates)\n",
    "7. [Handle Duplicates](#HandleDuplicates)\n",
    "8. [Export Cleaned Data](#Exporting)\n",
    "9. [Reflection](#Reflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d40c4b",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this exercise, Iâ€™ll check data consistency by finding mixed types, missing values, and duplicates in the wrangled data exported from Exercise 4.4. I'll then clean and export the cleaned data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8329ca",
   "metadata": {},
   "source": [
    "## Import Libraries & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "927c2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set path\n",
    "path = r'C:\\\\Users\\\\rhysm\\\\OneDrive\\\\Desktop\\\\Career Foundry\\\\Data Immersion\\\\Module 4\\\\04-2025 Instacart Basket Analysis'\n",
    "\n",
    "# Import data (from Exercise 4.4 export)\n",
    "df_ords = pd.read_csv(os.path.join(path, '02 Data', 'Prepared Data', 'orders_wrangled.csv'))\n",
    "df_prods = pd.read_csv(os.path.join(path, '02 Data', 'Prepared Data', 'products_wrangled.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0604a84",
   "metadata": {},
   "source": [
    "## Check for Mixed Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95eeaaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rhysm\\AppData\\Local\\Temp\\ipykernel_21392\\1105383265.py:3: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  mixed_check = df_ords[[col]].applymap(type).nunique()\n",
      "C:\\Users\\rhysm\\AppData\\Local\\Temp\\ipykernel_21392\\1105383265.py:3: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  mixed_check = df_ords[[col]].applymap(type).nunique()\n",
      "C:\\Users\\rhysm\\AppData\\Local\\Temp\\ipykernel_21392\\1105383265.py:3: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  mixed_check = df_ords[[col]].applymap(type).nunique()\n",
      "C:\\Users\\rhysm\\AppData\\Local\\Temp\\ipykernel_21392\\1105383265.py:3: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  mixed_check = df_ords[[col]].applymap(type).nunique()\n",
      "C:\\Users\\rhysm\\AppData\\Local\\Temp\\ipykernel_21392\\1105383265.py:3: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  mixed_check = df_ords[[col]].applymap(type).nunique()\n",
      "C:\\Users\\rhysm\\AppData\\Local\\Temp\\ipykernel_21392\\1105383265.py:3: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  mixed_check = df_ords[[col]].applymap(type).nunique()\n",
      "C:\\Users\\rhysm\\AppData\\Local\\Temp\\ipykernel_21392\\1105383265.py:3: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  mixed_check = df_ords[[col]].applymap(type).nunique()\n"
     ]
    }
   ],
   "source": [
    "# Check data types\n",
    "for col in df_ords.columns.to_list():\n",
    "    mixed_check = df_ords[[col]].applymap(type).nunique()\n",
    "    if mixed_check.values > 1:\n",
    "        print(f'{col} has mixed types')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fda3c6e",
   "metadata": {},
   "source": [
    "## Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fd6da00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                     0\n",
      "order_id                       0\n",
      "user_id                        0\n",
      "order_number                   0\n",
      "order_day_of_week              0\n",
      "order_hour_of_day              0\n",
      "days_since_prior_order    206209\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Find missing values\n",
    "missing = df_ords.isnull().sum()\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dcf9e8",
   "metadata": {},
   "source": [
    "## Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bfb7509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3214874, 7)\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing values (example)\n",
    "df_ords_clean = df_ords.dropna()\n",
    "print(df_ords_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c21b92",
   "metadata": {},
   "source": [
    "## Check for Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d47f979a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "dupes = df_ords_clean.duplicated()\n",
    "print(f'Total duplicates: {dupes.sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9be44c6",
   "metadata": {},
   "source": [
    "## Handle Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5f511c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3214874, 7)\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates\n",
    "df_ords_clean = df_ords_clean.drop_duplicates()\n",
    "print(df_ords_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517edcef",
   "metadata": {},
   "source": [
    "## Export Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9b72e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export cleaned data\n",
    "df_ords_clean.to_csv(os.path.join(path, '02 Data', 'Prepared Data', 'orders_checked.csv'))\n",
    "df_prods.to_csv(os.path.join(path, '02 Data', 'Prepared Data', 'products_checked.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a63c19",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "âœ… In this exercise, I checked and cleaned the wrangled data from Exercise 4.4 for mixed types, missing values, and duplicates, then exported the cleaned files for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e05a2d-a15d-4c11-ab42-ca62d7a72392",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
